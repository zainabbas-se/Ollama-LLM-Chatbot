## ü§ñ Ollama LLM Chatbot

A simple, production-ready **Streamlit interface** for chatting with a locally installed **Large Language Model (LLM)** using **Ollama**.  
It provides an easy-to-use web interface where users can type queries, view responses, and maintain chat history ‚Äî all running locally and privately.

---

## üöÄ Features

- **üß† Local Ollama LLM** ‚Äì Connects directly to a locally hosted Ollama LLM (e.g. `llama3`, `phi3`).
- **üí¨ Interactive Chat UI** ‚Äì Clean chat interface built with Streamlit.
- **üïí Conversation History** ‚Äì Previous messages displayed in an organized layout.
- **üîÑ Reset Chat** ‚Äì Button to clear previous chats and start fresh.
- **‚öôÔ∏è Model Selection** ‚Äì Choose from available Ollama models.
- **‚úÖ Connection Check** ‚Äì Verifies connectivity to the Ollama server automatically.

---

## üß© Requirements

- **Python** 3.9+
- **Ollama** (installed and running locally)
- **Python libraries**: `streamlit`, `requests` (installed via `requirements.txt`)

---

## üì¶ Installation & Setup

> All commands below use **bash** syntax. On Windows with PowerShell, just adjust path/activation accordingly.

### 1Ô∏è‚É£ Clone the Repository

```bash
git clone <YOUR_REPO_URL> "Ollama LLM Chatbot"
cd "Ollama LLM Chatbot"
```

### 2Ô∏è‚É£ Create and Activate a Virtual Environment

```bash
# Create venv
python -m venv .venv

# Activate venv (Linux / macOS)
source .venv/bin/activate

# Activate venv (Windows Git Bash)
source .venv/Scripts/activate
```

> On Windows PowerShell, you‚Äôd instead run:  
> `.\.venv\Scripts\Activate.ps1`

### 3Ô∏è‚É£ Install Python Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Install and Run Ollama

1. Download and install Ollama from the official site (`https://ollama.com`).
2. Pull your preferred model (example: `llama3.1`):

```bash
ollama pull llama3.1
```

3. Ensure the Ollama daemon is running in the background.

### 5Ô∏è‚É£ Configure Environment (Optional)

If your project uses environment variables (e.g. for data paths or model names), create a `.env` file:

```bash
cp .env.example .env
# then edit .env with your own settings
```

(If there is no `.env.example`, you can skip this step or create `.env` based on your own needs.)

### 6Ô∏è‚É£ Run the App

```bash
streamlit run app.py
```

Then open the URL shown in your terminal (usually `http://localhost:8501`) in your browser.

---

## üí° How It Works

- **Ingestion & Indexing**
  - Your documents are loaded and split into chunks.
  - Each chunk is converted into an **embedding vector** and stored in a lightweight vector index.

- **Query Processing**
  - When you ask a question, it‚Äôs embedded and used to search the index.
  - The most relevant chunks are retrieved as **context** for the LLM.

- **RAG Generation**
  - The retrieved context and your question are sent to the Ollama model.
  - The LLM generates a grounded answer that reflects your underlying documents.

- **Web UI Layer (Streamlit + Tailwind CSS v4)**
  - Streamlit handles user interaction (inputs, outputs, session state).
  - Tailwind CSS v4 powers the modern styling for the chat layout and controls.

You can customize the **retrieval strategy**, **chunk sizes**, and **prompt templates** in the Python code (primarily in `app.py` and related modules).

---

## üë®‚Äçüíª Author

- Zain Abbas
- GitHub: https://github.com/zainabbas-se
- LinkedIn: https://www.linkedin.com/in/zainabbas-se/

If you use this project as a starter template or extend it, ‚≠ê **star the repo** and feel free to open issues or PRs!
